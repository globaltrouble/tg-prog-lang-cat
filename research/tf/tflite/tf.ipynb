{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63c78449-25ce-46e8-91b7-533b7cca8f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sun/prog/tg-prog-lang-cat/research/tf/tflite/.venv/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/sun/prog/tg-prog-lang-cat/research/tf/tflite/.venv/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.4 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "/home/sun/prog/tg-prog-lang-cat/research/tf/tflite/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import text_classifier\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker.text_classifier import AverageWordVecSpec\n",
    "from tflite_model_maker.text_classifier import DataLoader\n",
    "\n",
    "from tflite_support.task import core\n",
    "from tflite_support.task import processor\n",
    "from tflite_support.task import text\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "# tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1c3500d-7247-4bca-983f-de883e70d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = tf.keras.utils.get_file(\n",
    "      fname='SST-2.zip',\n",
    "      origin='https://dl.fbaipublicfiles.com/glue/data/SST-2.zip',\n",
    "      extract=True)\n",
    "data_dir = os.path.join(os.path.dirname(data_dir), 'SST-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "709b6468-9279-49bd-8d7d-de11a7a6037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def replace_label(original_file, new_file):\n",
    "  # Load the original file to pandas. We need to specify the separator as\n",
    "  # '\\t' as the training data is stored in TSV format\n",
    "  df = pd.read_csv(original_file, sep='\\t')\n",
    "\n",
    "  # Define how we want to change the label name\n",
    "  label_map = {0: 'negative', 1: 'positive'}\n",
    "\n",
    "  # Excute the label change\n",
    "  df.replace({'label': label_map}, inplace=True)\n",
    "\n",
    "  # Write the updated dataset to a new file\n",
    "  df.to_csv(new_file)\n",
    "\n",
    "# Replace the label name for both the training and test dataset. Then write the\n",
    "# updated CSV dataset to the current folder.\n",
    "replace_label(os.path.join(os.path.join(data_dir, 'train.tsv')), 'train.csv')\n",
    "replace_label(os.path.join(os.path.join(data_dir, 'dev.tsv')), 'dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f5e3c9-b524-47d1-a3b0-e3fb1faf2672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 15:05:24.512973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-14 15:05:24.555823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-14 15:05:24.556079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "mb_spec = model_spec.get('mobilebert_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f5a3dbe-72a1-495c-ae24-b00e0f980c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 15:05:50.294346: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-14 15:05:50.295800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-14 15:05:50.296509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-14 15:05:50.296729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-14 15:05:50.958310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-14 15:05:50.958785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-14 15:05:50.959156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-14 15:05:50.959472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3384 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 950M, pci bus id: 0000:01:00.0, compute capability: 5.0\n"
     ]
    }
   ],
   "source": [
    "train_data = DataLoader.from_csv(\n",
    "      filename='train.csv',\n",
    "      text_column='sentence',\n",
    "      label_column='label',\n",
    "      model_spec=mb_spec,\n",
    "      is_training=True)\n",
    "test_data = DataLoader.from_csv(\n",
    "      filename='dev.csv',\n",
    "      text_column='sentence',\n",
    "      label_column='label',\n",
    "      model_spec=mb_spec,\n",
    "      is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12eb8d67-a0b5-47ae-a297-ce2ac4fd152b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27ab6d2f-0dfd-48e4-9531-21ff952dffdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.framework.config.set_logical_device_configuration(device, logical_devices)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.set_logical_device_configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea37e376-3eb1-44a1-a439-c01e3704be4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtext_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'average_word_vec'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdo_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Loads data and train the model for test classification.\n",
       "\n",
       "Args:\n",
       "  train_data: Training data.\n",
       "  model_spec: Specification for the model.\n",
       "  validation_data: Validation data. If None, skips validation process.\n",
       "  batch_size: Batch size for training.\n",
       "  epochs: Number of epochs for training.\n",
       "  steps_per_epoch: Integer or None. Total number of steps (batches of\n",
       "    samples) before declaring one epoch finished and starting the next\n",
       "    epoch. If `steps_per_epoch` is None, the epoch will run until the input\n",
       "    dataset is exhausted.\n",
       "  shuffle: Whether the data should be shuffled.\n",
       "  do_train: Whether to run training.\n",
       "\n",
       "Returns:\n",
       "  An instance based on TextClassifier.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/prog/tg-prog-lang-cat/research/tf/tflite/.venv/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/text_classifier.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = text_classifier.create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54356846-d71a-4161-aad6-8c286c1b7e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Retraining the models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Retraining the models...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8418/8418 [==============================] - 4533s 531ms/step - loss: 0.3912 - test_accuracy: 0.8860\n",
      "Epoch 2/2\n",
      "8418/8418 [==============================] - 4466s 530ms/step - loss: 0.1547 - test_accuracy: 0.9614\n",
      "Epoch 3/3\n",
      "8418/8418 [==============================] - 4481s 532ms/step - loss: 0.0854 - test_accuracy: 0.9808\n"
     ]
    }
   ],
   "source": [
    "model = text_classifier.create(train_data, model_spec=mb_spec, epochs=3, shuffle=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "317404b1-3448-4d54-a450-5e8000b00658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_word_ids (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " input_mask (InputLayer)        [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " input_type_ids (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " hub_keras_layer_v1v2 (HubKeras  (None, 512)         24581888    ['input_word_ids[0][0]',         \n",
      " LayerV1V2)                                                       'input_mask[0][0]',             \n",
      "                                                                  'input_type_ids[0][0]']         \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512)          0           ['hub_keras_layer_v1v2[0][0]']   \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 2)            1026        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 24,582,914\n",
      "Trainable params: 24,582,914\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "674d9d05-3393-4680-b8df-b599c723e5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 24s 623ms/step - loss: 0.4501 - test_accuracy: 0.9209\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb0d5edd-9b4b-442f-811f-428d4d5ad4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 18:51:26.566015: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpum2wz7am/saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpum2wz7am/saved_model/assets\n",
      "2023-11-14 18:52:00.865952: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-11-14 18:52:00.865981: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-11-14 18:52:00.867377: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpum2wz7am/saved_model\n",
      "2023-11-14 18:52:00.916730: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-11-14 18:52:00.916763: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpum2wz7am/saved_model\n",
      "2023-11-14 18:52:01.124872: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-11-14 18:52:04.777890: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpum2wz7am/saved_model\n",
      "2023-11-14 18:52:07.285839: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 6418478 microseconds.\n",
      "2023-11-14 18:52:09.263375: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-14 18:52:15.569275: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 5.511 G  ops, equivalently 2.755 G  MACs\n",
      "\n",
      "2023-11-14 18:52:15.745712: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_0/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.745770: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_0/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.745777: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_0/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.745783: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_0/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.745790: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_0/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.745795: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_0/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.745801: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_0/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.745827: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_0/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.745861: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_1/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.745867: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_1/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.745873: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_1/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.745878: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_1/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.745884: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_1/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.745890: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_1/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.745914: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_1/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.745921: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_1/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.745937: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_2/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.745964: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_2/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.745970: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_2/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.745975: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_2/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.745981: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_2/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.745987: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_2/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746010: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_2/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746016: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_2/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746030: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_3/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746035: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_3/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746041: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_3/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746046: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_3/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746052: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_3/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746057: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_3/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746079: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_3/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746084: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_3/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746100: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_4/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746106: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_4/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746111: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_4/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746117: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_4/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746122: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_4/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746127: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_4/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746133: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_4/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746137: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_4/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746150: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_5/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746155: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_5/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746160: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_5/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746166: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_5/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746172: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_5/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746177: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_5/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746182: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_5/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746187: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_5/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746201: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_6/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746206: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_6/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746211: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_6/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746217: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_6/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746223: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_6/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746228: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_6/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746233: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_6/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746239: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_6/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746253: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_7/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746259: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_7/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746264: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_7/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746269: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_7/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746275: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_7/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746280: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_7/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746286: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_7/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746291: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_7/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746315: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_8/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746321: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_8/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746326: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_8/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746331: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_8/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746337: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_8/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746342: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_8/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746347: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_8/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746353: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_8/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746366: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_9/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746371: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_9/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746377: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_9/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746382: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_9/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746388: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_9/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746394: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_9/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746399: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_9/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746404: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_9/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746419: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_10/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746424: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_10/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746429: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_10/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746435: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_10/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746442: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_10/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746447: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_10/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746452: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_10/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746457: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_10/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746469: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_11/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746474: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_11/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746479: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_11/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746484: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_11/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746490: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_11/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746495: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_11/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746500: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_11/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746506: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_11/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746519: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_12/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746524: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_12/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746529: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_12/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746534: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_12/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746540: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_12/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746545: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_12/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746550: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_12/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746555: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_12/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746567: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_13/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746573: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_13/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746577: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_13/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746582: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_13/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746589: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_13/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746594: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_13/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746599: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_13/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746604: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_13/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746617: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_14/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746622: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_14/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746628: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_14/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746633: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_14/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746639: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_14/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746644: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_14/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746650: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_14/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746655: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_14/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746677: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_15/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746695: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_15/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746702: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_15/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746717: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_15/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746724: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_15/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746730: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_15/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746735: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_15/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746740: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_15/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746753: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_16/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746759: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_16/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746764: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_16/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746769: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_16/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746775: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_16/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746780: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_16/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746786: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_16/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746792: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_16/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746805: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_17/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746811: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_17/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746816: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_17/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746821: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_17/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746827: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_17/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746832: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_17/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746837: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_17/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746842: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_17/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746855: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_18/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746860: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_18/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746866: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_18/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746871: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_18/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746878: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_18/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746883: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_18/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746911: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_18/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746917: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_18/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746948: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_19/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746954: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_19/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746958: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_19/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746964: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_19/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746970: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_19/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.746975: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_19/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747000: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_19/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747007: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_19/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747021: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_20/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747045: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_20/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747050: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_20/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747055: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_20/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747062: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_20/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747067: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_20/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747073: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_20/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747078: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_20/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747090: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_21/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747095: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_21/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747100: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_21/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747106: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_21/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747112: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_21/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747117: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_21/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747122: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_21/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747128: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_21/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747140: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_22/attention/self/MatMul10 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747146: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_22/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747151: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_22/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747156: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_22/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747162: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_22/attention/self/MatMul_114 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747188: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_22/attention/self/MatMul_116 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747194: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_22/attention/self/MatMul_118 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747199: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_22/attention/self/MatMul_120 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747213: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_23/attention/self/MatMul11 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747218: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_23/attention/self/MatMul12 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747223: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_23/attention/self/MatMul13 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747249: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_23/attention/self/MatMul14 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747274: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_23/attention/self/MatMul_121 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747280: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_23/attention/self/MatMul_123 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747285: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_23/attention/self/MatMul_125 because it has no allocated buffer.\n",
      "2023-11-14 18:52:15.747290: I tensorflow/lite/tools/optimize/quantize_weights.cc:234] Skipping quantization of tensor bert/encoder/layer_23/attention/self/MatMul_127 because it has no allocated buffer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Vocab file and label file are inside the TFLite model with metadata.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Vocab file and label file are inside the TFLite model with metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saved vocabulary in /tmp/tmpo73ns1gv/vocab.txt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saved vocabulary in /tmp/tmpo73ns1gv/vocab.txt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving labels in /tmp/tmpo73ns1gv/labels.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving labels in /tmp/tmpo73ns1gv/labels.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished populating metadata and associated file to the model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished populating metadata and associated file to the model:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:mobilebert-default/model.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:mobilebert-default/model.tflite\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:The associated file that has been been packed to the model is:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:The associated file that has been been packed to the model is:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:['vocab.txt', 'labels.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:['vocab.txt', 'labels.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:TensorFlow Lite model exported successfully: mobilebert-default/model.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:TensorFlow Lite model exported successfully: mobilebert-default/model.tflite\n"
     ]
    }
   ],
   "source": [
    "model.export(export_dir='mobilebert-default/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5df1b808-6440-4dca-bf4d-ea7775f9486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saved vocabulary in mobilebert-label-vocab/vocab.txt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saved vocabulary in mobilebert-label-vocab/vocab.txt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving labels in mobilebert-label-vocab/labels.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving labels in mobilebert-label-vocab/labels.txt\n"
     ]
    }
   ],
   "source": [
    "model.export(export_dir='mobilebert-label-vocab/', export_format=[ExportFormat.LABEL, ExportFormat.VOCAB])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
